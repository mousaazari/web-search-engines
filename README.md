# A simulation of term-based and document-based index partitioning methods
We evaluate the system's performance at each node (partition) and compute query processing costs for each input from the dataset. The implementation and the experiments are developed on the Google Colab environment and Python programming language. 
## Implementation Details
In preprocessing step, after reading each file and extracting the necessary information, I store the queries, lengths of the posting lists, and the terms in different lists which will be used for creating the partitions and evaluating the model. 
To create the partitions, I define K as the number of nodes and donate the terms and their posting list length as key-value pairs using a round-robin algorithm to these K different nodes.
Round-robin is a scheduling algorithm used to allocate a fixed time slot for each process in a system with multiple processes. It works by maintaining a queue of processes and assigning a fixed time quantum to each process. The system selects the first process in the queue and allocates the CPU to it for the time quantum. If the process finishes within the time quantum, it is removed from the queue and the next process is given the CPU. If the process does not finish within the time quantum, it is preempted and added to the end of the queue. This process is repeated until all the processes in the queue have been executed. The advantage of Round Robin is that it ensures that all processes get a fair share of the CPU time, but it can suffer from overhead due to the overhead of preempting and resuming processes.
To solve this problem, the code iterates through each term in the query and checks which dictionary it belongs to. It is designed to do this by looping through the dictionaries and checking if the term is a key in each dictionary. When it finds the dictionary that the term belongs to, it retrieves the corresponding value for the term and stores it in a result list.
The code involves an inference function that takes a list of terms (query) and a list of dictionaries as input and returns a list of the corresponding values for each term in the query. If the terms of a particular query do not appear in any of the dictionaries, the function will discard the query.
The script then checks if the dictionary has only one key. If that is the case, it gets the key and value and checks if the value is a list. If the value is a list, the script iterates over the values in the list and checks if each value is an integer or a string that can be converted to an integer. If the value is an integer or a string that can be converted to an integer, it is added to a sum. At the end, the script prints the sum of all values. If the value is not a list, the script simply prints the value.
If the dictionary has more than one key, the script iterates over the dictionary and checks if each value is a list with more than one element. If the value is a list with more than one element, the script prints the minimum value. If the value is not a list or has only one element, the script prints the value. The motivation behind choosing dictionaries to use as nodes is their time efficiency where we can access the values in O(1) time. The overall procedure is very similar for both term-based and document-based partitioning.
